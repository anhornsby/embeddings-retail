<!doctype html>
<!-- 
Reveal Presentation
internal dunnhumby training (not for public release)
Created by Adam Hornsby
-->

<!-- 
Huge thanks to the following people for lending their diagrams:

Embeddings - http://gpucomputing.shef.ac.uk/education/intro_dl_sharc_dgx1/lab06/
word2vec - https://www.tensorflow.org/tutorials/word2vec
King-Queen vectors - http://signaldatascience.com/projects/oliviaschaefer/

 -->

<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Product Embeddings - Adam Hornsby</title>

		<link rel="stylesheet" href="lib/css/zenburn.css">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">
		<link rel="stylesheet" href="css/vampiriser/vampiriser.css">

		<!-- dh Logo Watermark -->
		<div id="myLogo" style="background: url(./images/dh_logo2.png);
        position: absolute;
        bottom: 50px;
        right: 150px;
        z-index: 1;
        width: 120px;
        background-size: cover;
        height: 30px"></div>

		<div id="myLogo" style="
        position: absolute;
        bottom: 45px;
        left: 100px;
        z-index: 1;
        width: 200px;
        background-size: cover;
        height: 30px"><font face="arial" size="2" color="#363534">&copy; dunnhumby 2018</font></div>

<!-- 		<div style="
        position: relative;
        bottom: 48px;
        left: 10px;
        z-index: 1;"><font face="arial" size="2" color="#363534">&copy; dunnhumby 2017 | confidential</font></div>
 -->
		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-background="#009B74">
					<h3> Understanding customers better through neural network embeddings </h3>
					<small>
						Adam Hornsby <br>
						(adam.hornsby@dunnhumby.com)
					</small>
				</section>

				<section>
					<h4> About Me </h4>
					<small>
					<ul>
						<br>
						<span class="fragment"><li>Senior Data Scientist @ dunnhumby (4.5 years)</li></span>
						<span class="fragment"><li><b>Researching and deploying</b> machine learning at scale</li></span>
						<span class="fragment"><li>Experimental Psychology, PhD Student @ UCL (part-time)</li></span>
						<span class="fragment"><li>Researching <b>consumer decision-making</b> through experiments, big-data and computational modelling</li></span>
					</ul>
					</small>
				</section>

				<section>
					<h3> Extensive heritage and experience working with retailers and brands </h3>
					<small>
					<ul>
							<img width=2000 data-src="./images/map.png" style= "border-style: none; box-shadow: none">
					</ul>
					</small>
				</section>

<!-- 				<section>
					<small>
					<ul>
							<img width=800 data-src="./images/what.png" style= "border-style: none; box-shadow: none">
					</ul>
					</small>
				</section> -->

				<section>
					<h3> Today </h3>
					<small>
					<ol>
						<br>
						<span class="fragment"><li>Why <b>item similarity</b> is so important in retail science</li></span>
						<span class="fragment"><li>What are <b>embeddings</b>?</li></span>
						<span class="fragment"><li>How can <b>2vec</b> algorithms help?</li></span>
						<span class="fragment"><li>Conclusions</li></span><br><br>

						<span class="fragment">(Massive thanks to <a href='https://www.linkedin.com/in/joshua-cooper-07b72953/'>Josh Cooper</a> for doing lots of the thinking in this presentation)</span>
					</ol>
					</small>
				</section>

				<section data-background="#A31A7E">

					<h1> Item similarity </h1>
					<small>

					</small>
				</section>

				<section>
					<h3> How similar are these two products? </h3>
					<small>
					<ul>
							<img width=200 data-src="./images/cat.jpg" style= "border-style: none; box-shadow: none"><img width=200 data-src="./images/dog.jpg" style= "border-style: none; box-shadow: none">
					</ul>
					<br><br>
					<!-- <span class="fragment">How similar are these products?</span><br><br> -->
					<span class="fragment">Questions of similarity are <b>everywhere in retail:</b></span><br><br>
					<span class="fragment">"Your selected product X <b>goes well with</b> product Y" (i.e. product complementarity)</span><br>
					<span class="fragment">"Your product X <b>was not available, so how about alternative</b> Y?" (i.e. product substitutability)</span><br>
					<span class="fragment">"<b>Products of this type</b> tend to be placed together on the shelf" (i.e. product similarity)</span><br>
					<span class="fragment">"<b>Customers like you</b> also tend to buy Y" (i.e. customer similarity)</span><br><br>

					<span class="fragment">Solving similarity is a <b>huge goal for data scientists</b> working to improve; recommendations, ranging, pricing, assortment and more</span><br><br>

					</small>
				</section>

				<section>
					<h5> Traditional methods don't help with similarity </h5>
					<small>
					<ul>
							<img width=400 data-src="./images/onehotencode.png" style= "border-style: none; box-shadow: none">
					</ul>
					<br><br>
					<span class="fragment">Most scientists will use <b>dummy (or one-hot) encoding</b> to represent categorical data</span><br><br>
					<span class="fragment">Dummy coded data is <b>too sparse</b> (i.e. too many zeros)<br>
					<span class="fragment">Any similarity measure will tell you that <i>cat food</i> and <i>dog food</i> are <b>totally unrelated</b> using this approach</span><br><br>

					<span class="fragment">We need a way to represent these items using <b>dense</b> vectors</span><br><br>

					</small>
				</section>

				<section data-background="#E17000">
					<h1> Embeddings </h1>
				</section>

				<section>
					<h3> Embeddings are dense representations </h3>
					<small>
					<ul>
							<img width=600 data-src="./images/embedding.png" style= "border-style: none; box-shadow: none">
					</ul>
					<br>

					<span class="fragment">Traditionally used in <b>natural language processing (NLP)</b></span><br>
					<span class="fragment">Learned <b>dense representations</b> of otherwise sparse data (vector size is a parameter)</span><br>

					<span class="fragment">Typically learnt with <b>neural networks</b></span><br><br>

					<span class="fragment">When learned well, produce <b>similar vectors for similar items</b> (e.g. cats vs. dogs)</span><br><br>

					<span class="fragment"><a href="https://deeplearning4j.org/thoughtvectors">Geoff Hinton</a> has argued that <b>all thoughts can be represented</b> with embedding vectors</span><br><br>


					</small>
				</section>

				<section>
					<h3> *2vec Algorithms </h3>
					<small>
					<ul>
							<img width=400 data-src="./images/word2vec.png" style= "border-style: none; box-shadow: none">
					</ul>
					<br><br>

					<span class="fragment">A set of <b>unsupervised learning</b> algorithms that learn vector embeddings (Mikolov et al.)</span><br><br>
					<span class="fragment"><b>word2vec</b> used to learn vector embeddings for <b>words</b></span><br>
					<span class="fragment"><b>doc2vec</b> used to learn vector embeddings for <b>documents</b> (e.g. sentences)</span><br><br>

					<span class="fragment">Create <b>lots of little classification problems</b> (e.g. use vector from one word to predict next word in sentence, then backprop) </span><br>
<!-- 					<span class="fragment">Embedding vectors are then <b>trained using backpropagation</b> in each iteration </span><br><br> -->
					<span class="fragment">Typically <b>very fast to train</b> and scale well to large data</span><br>

					</small>
				</section>

				<section>
					<!-- Show how they're related -->
					<h3> Sentences vs. Baskets </h3>
					<small>
					<ul>
							<img width=500 data-src="./images/documents.png" style= "border-style: none; box-shadow: none">
					</ul>
					<br><br>

					<span class="fragment">NLP algorithms (like doc2vec) assume that you have <b>words (i.e. items) and sentences (i.e. documents)</b></span><br><br>

					<span class="fragment"><b>Products</b> are analogous to words and <b>baskets/customers</b> are analogous to sentences</span><br>
					<span class="fragment">This data is a <b>neat place to test NLP algorithms</b> (e.g. no order, "stop words" and arguably more natural)</span><br><br>

					<span class="fragment">e.g. A basket can be represented as a vector of product codes or descriptions</span><br>

					</small>
					<span class="fragment">
					<pre class="python" ><code data-trim>
["Rice crispies", "Tomato ketchup", "Peanut butter"]
</code>
</pre>
					
</span>	
				
				</section>


 				<section data-background="#363534">
					<h1> The model </h1>
				</section> 

				<section>

					<h3>The modelling problem</h3>
					<small>
					<ul>
							<img width=300 data-src="./images/source.png" style= "border-style: none; box-shadow: none">
					</ul>
					<br><br>
					<span class="fragment"><b>Aim:</b> Understand whether doc2vec can learn <b>useful</b> vector representations of products and baskets</span><br><br>
					<span class="fragment">Model developed on <b>free transactional</b> dataset from <a href="https://www.dunnhumby.com/sourcefiles">dunnhumby source files</a></span><br>
					<span class="fragment">2 years worth of transactions from <b>2500 households</b></span><br><br>

					<span class="fragment">Using <a href="https://radimrehurek.com/gensim/">gensim</a> implementation of doc2vec (Python)</span><br>

					<span class="fragment">Learn product and document vectors at same time</span>
					<br><br>

					<span class="fragment">Trained on GCP Compute Engine (4 vCPUs and 15GB memory) in (~10 minutes per experiment)</span><br>
					<span class="fragment">Model solutions evaluated using Tensorflow's amazing <a href="https://github.com/tensorflow/embedding-projector-standalone">Embedding Explorer</a></span>

				</small>

				</section>

				<section data-background="#B19B00">
					<h1> Evaluation </h1>
				</section>

				<section data-background-video="./images/home2vec.mov" data-background-size="100px" data-background-video-loop data-background-repeat="repeat">
				</section>

				<section data-background-video="./images/easter.mov" data-background-size="100px" data-background-video-loop data-background-repeat="repeat">
				</section>

				<section data-background-video="./images/vegetarian.mov" data-background-size="100px" data-background-video-loop data-background-repeat="repeat">
				</section>

				<section data-background-video="./images/basket.mov" data-background-size="100px" data-background-video-loop data-background-repeat="repeat">
				</section>

				<section>
					<h3>Analogical reasoning</h3>
					<small>
					<ul>
						<img width=500 data-src="./images/king.png" style= "border-style: none; box-shadow: none">
					</ul><br><br>

					<span class="fragment"><b>Semantic relationships between words</b> are typically preserved within embedding space</span><br><br>
					<span class="fragment">$King - Man + Woman = y$</span><br><br>
					<span class="fragment">Vectors most similar to $y$ tend to be <b>related to "queen"</b></span><br><br>

					<span class="fragment">What would be the equivalent using <b>product embeddings</b>?</span><br>
					<!-- <span class="fragment">(The following examples have <b>not</b> been cherry picked!)</span> -->

				</small>
				</section> 


				<section>
					<small>
						<h2>Identifying cheaper alternatives</h2>
						<b>Premium Meat - Premium + Economy = ?</b><br>
						<br>
						<span class="fragment">
					<table>
						<thead>
							<tr>
								<th>Rank</th>
								<th>Product description</th>
								<th>Cosine similarity</th>
								<!-- <th>Quantity</th> -->
							</tr>
						</thead>
						<tbody>
							<tr>
								<th>1</th>
								<td>Economy meat</td>
								<td>0.542439</td>
								<!-- <td>7</td> -->
							</tr>
							<tr>
								<th>2</th>
								<td>Margarine stick</td>
								<td>0.518072</td>
								<!-- <td>18</td> -->
							</tr>
							<tr>
								<th>3</th>
								<td>Carrots (bagged)</td>
								<td>0.500584</td>
								<!-- <td>2</td> -->
							</tr>
							<tr>
								<th>4</th>
								<td>Chocolate milk</td>
								<td>0.489882</td>
								<!-- <td>2</td> -->
							</tr>
						</tbody>
					</table>
				</span>
					<br>
					<br>
					<span class="fragment">The most similar product to the target vector is <b>economy meat</b>. Other items are also cheaper alternatives.</span><br>
					<span class="fragment">Suggests that there is a <b>"price sensitivity"</b> direction within the space</span>
				</small>
				</section>

				<section>
					<small>
						<h2>Identifying vegetarian alternatives</h2>
						<b>Frozen Burgers - Beef + Tofu = ?</b><br><br>
						<span class="fragment">
					<table>
						<thead>
							<tr>
								<th>Rank</th>
								<th>Product description</th>
								<th>Cosine similarity</th>
								<!-- <th>Quantity</th> -->
							</tr>
						</thead>
						<tbody>
							<tr>
								<th>1</th>
								<td>Frozen entrees</td>
								<td>0.518292</td>
								<!-- <td>7</td> -->
							</tr>
							<tr>
								<th>2</th>
								<td>Frozen meat (vegetarian)</td>
								<td>0.484718</td>
								<!-- <td>18</td> -->
							</tr>
							<tr>
								<th>3</th>
								<td>Frozen meal combo/dinners</td>
								<td>0.464290</td>
								<!-- <td>2</td> -->
							</tr>
							<tr>
								<th>4</th>
								<td>Eggs</td>
								<td>0.462903</td>
								<!-- <td>2</td> -->
							</tr>
						</tbody>
					</table>
				</span>
					<br>
					<br>
					<span class="fragment">The most similar products to the target vector tend to be <b>frozen and/or meat free</b></span><br>
					<span class="fragment">Suggests there are <b>"frozen"</b> and <b>"vegetarian"</b> directions within the embedding space</span>
				</small>
				</section>

				<section data-background="#009B74">
					<h1>Conclusions</h1>
				</section>

				<section>
					<h3>The model looks great</h3>
					<!-- <br> -->
					<video loop data-autoplay src="./images/showcase.mov" width=600></video><br>
					<small>
						<br>
						<span class="fragment">+ Very <b>visual</b> application of machine learning in retail</span><br>
					<span class="fragment">+ The model appears to <b>understand product similarity</b> well</span><br>
					<span class="fragment">+ <b>Analogical reasoning</b> results seem intuitive</span><br>
					<span class="fragment">+ <b>Scales</b> better than rival algorithms (e.g. NMF)</span><br><br>

					<span class="fragment">- Need some more <b>automated</b> ways of <b>evaluating</b> the model</span><br>
					<span class="fragment">- Need <b>less homogenous</b> way of representing baskets & customers</span><br>
					<span class="fragment">- <b>Interpretation of "similar"</b> is slightly unusual (neither comps nor subs)</span><br>

				    </small>

				</section>

				<section>
					<h3>This model has many applications</h3>
					<small>
					<ul>
						<img width=800 data-src="./images/choice.png" style= "border-style: none; box-shadow: none">
					</ul>
					<br>

					<span class="fragment"><b>Global deployment</b> of model in collaboration with partners</span><br><br>

					<span class="fragment"><b>Ultra-fast deployment</b> of basket and customer <b>segmentations</b> with new clients</span><br>
					<span class="fragment">Exploring use of model to suggest <b>alternative & supplementary</b> products in ranges during <b>store ranging</b></span><br>
					<span class="fragment">Exploring power of new features in <b>personalised recommender systems</b></span><br><br>

					<span class="fragment">A <b>significant leap forward</b> in the endeavour to describe <b>similarity</b></span>
					

				    </small>

				</section>

				<section  data-background="#363534">
					<h1> Thank You </h1>
					<small>@adamnhornsby <br>
						adam.hornsby@dunnhumby.com<br><br>


					<b>dunnhumby.com/careers</b>
					</small>
				</section>


			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

	<script type="text/javascript">
	
		$(document).ready(function(){
			Reveal.addEventListener( 'slidechanged', function( event ) {
				// event.previousSlide, event.currentSlide, event.indexh, event.indexv
				var recSlide = event.currentSlide;

				// check if is youtube url is inside slide and save youtubeID, prevent reloading checking videoframe
				if( ($(recSlide).find('.videoyt').length != 0) && ($(recSlide).find('.videoframe').html()=='') ) {
					ytid = $(recSlide).find('.videoyt').attr('data-youtube');
					// replace a.videoyt with video embed code
					$(recSlide).find('.videoframe').html('<iframe width="640" height="480" src="//www.youtube.com/embed/'+ytid+'?wmode=opaque&amp;rel=0&amp;vq=large" frameborder="0" allowfullscreen></iframe>');
					// &iv_load_policy=<3></3>
					// give anchor with videoid a correct link
					$(recSlide).find('.videoyt').attr('href', '//www.youtube.com/watch?v='+ytid);
					$(recSlide).find('.videoyt').attr('target', '_blank');
				}
			});
		});
		
	</script>


		<script>

			// MathJax.Hub.Config({
			//   tex2jax: {
			//          inlineMath: [ ['$','$'], ['\\(','\\)'] ]
			//   }
			// 	}),
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
				transition: 'linear',

				math: {
					// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
					config: 'TeX-AMS_HTML-full'
				},
				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
